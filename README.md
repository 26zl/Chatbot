RAG Chatbot
===========================

Minimal RAG pipeline that crawls Cloudflare docs, stores embeddings in Pinecone via Cohere, and serves a Streamlit chatbot for interactive Q&A.

Security notes (read before public deployment)
---------------------------------------------
This project can scrape user-provided URLs and uses paid API keys. If you deploy it somewhere reachable by others, set the safety controls:
* `APP_PASSWORD` to require a password before using the UI
* `ALLOWED_DOMAINS` to restrict which sites can be scraped (recommended)
* `SCRAPE_LIMIT_PER_HOUR` / `QUERY_LIMIT_PER_MINUTE` to limit spend/abuse

Project layout
--------------
```
.
├── data/              # Generated artifacts (crawl output, logs)
├── .streamlit/        # Streamlit secrets (only needed for Docker)
│   └── secrets.toml
├── .env               # Environment variables (used for all local operations)
├── app.py             # Streamlit chat UI (retrieval + answer generation)
├── crawler.py         # BFS crawler for scraping documentation
├── build_index.py     # Embeds scraped text into Pinecone
├── index_utils.py     # Helper utilities for embedding and upserting
├── requirements.txt
├── Dockerfile
├── compose.yaml       # Docker Compose configuration
└── README.md
```

Prerequisites
-------------
* Python 3.11+ (3.9+ works)
* API keys (required):
  - `COHERE_API_KEY` - for embedding generation
  - `PINECONE_API_KEY` - for vector storage
  - `OPENAI_API_KEY` - for answer generation
* Optional: `PINECONE_INDEX` (defaults to "website-knowledge-index")

Setup & Database Creation
--------------------------
**Important:** You must build your own database before running the chatbot. The application does not work without indexed data in Pinecone.

### Step 1: Install dependencies
```bash
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

### Step 2: Create `.env` file
Create a `.env` file in the project root with your API keys:

```bash
cat > .env << EOF
COHERE_API_KEY=your_cohere_key_here
PINECONE_API_KEY=your_pinecone_key_here
OPENAI_API_KEY=your_openai_key_here
PINECONE_INDEX=website-knowledge-index
EOF
```

This file is used by all local operations: `crawler.py`, `build_index.py`, and `streamlit run app.py`.

### Step 3: Build the database
```bash
# Scrape documentation (creates data/crawled.jsonl)
python crawler.py

# Embed and upload to Pinecone (requires API keys from .env)
python build_index.py
```

This process:
1. Crawls up to 200 pages from Cloudflare Workers documentation
2. Extracts and cleans text content
3. Generates embeddings using Cohere
4. Uploads vectors to your Pinecone index

**Note:** The `build_index.py` script will automatically create the Pinecone index if it doesn't exist. You can modify `START_URL` and `MAX_PAGES` in `crawler.py` to scrape different documentation or adjust the number of pages.

### Step 4: Run the chatbot locally
```bash
streamlit run app.py
```

The chatbot will be available at `http://localhost:8501`

**Note:** Running locally with `streamlit run app.py` uses the `.env` file. You do NOT need `.streamlit/secrets.toml` for local development.

Local workflow summary
----------------------
```bash
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 1. Create .env file
cat > .env << EOF
COHERE_API_KEY=your_key
PINECONE_API_KEY=your_key
OPENAI_API_KEY=your_key
PINECONE_INDEX=website-knowledge-index
EOF

# 2. Build database
python crawler.py
python build_index.py

# 3. Run chatbot
streamlit run app.py
```

All local operations use the `.env` file. You're done! 

Artifacts
---------
* `data/crawled.jsonl` - scraped content (created by crawler.py)
* `data/query_log.jsonl` - query history (generated by Streamlit app during use)
* `data/evaluation_results.jsonl` - timing metrics (if running locally with timing enabled)
* `.env` - API keys (do not commit!)

Docker Deployment (Optional)
-----------------------------
If you want to run the application in Docker, you need an additional configuration file.

### Why Docker needs `.streamlit/secrets.toml`
Docker requires Streamlit secrets in TOML format. When running locally with `streamlit run`, the app uses `.env`, but inside Docker containers, Streamlit expects secrets in `.streamlit/secrets.toml`.

### Docker Setup

**Prerequisites:**
1. Build the database first (run steps 1-3 above)
2. Ensure your Pinecone index is populated

**Step 1: Create `.streamlit/secrets.toml`**

```bash
mkdir -p .streamlit
cat > .streamlit/secrets.toml << EOF
COHERE_API_KEY = "your_cohere_key_here"
PINECONE_API_KEY = "your_pinecone_key_here"
OPENAI_API_KEY = "your_openai_key_here"
PINECONE_INDEX = "website-knowledge-index"
EOF
```

This file contains the exact same values as `.env`, just in TOML syntax for Streamlit. It is listed in `.gitignore`, so never commit the real secrets—keep only the checked-in `.streamlit/secrets.toml.example`.

**Important:** Use the **same API keys** as in your `.env` file, just formatted differently.

**Step 2: Run with Docker Compose**

```bash
docker compose up --build
```

That's it! The `compose.yaml` file is already configured with all necessary volume mounts and settings.

The chatbot will be available at `http://localhost:8501`
(`compose.yaml` binds the port to `127.0.0.1` so it's only reachable locally by default.)

To stop:
```bash
docker compose down
```

### Manual Docker build and run (alternative)

If you prefer to build and run manually without Docker Compose:

```bash
# Build the image
docker build -t rag-docs-bot .

# Run the container (Linux/macOS/WSL)
docker run --rm -p 127.0.0.1:8501:8501 \
  -v "$PWD/data:/app/data" \
  -v "$PWD/.streamlit:/app/.streamlit:ro" \
  rag-docs-bot

# Run the container (PowerShell)
docker run --rm -p 127.0.0.1:8501:8501 `
  -v "${PWD}/data:/app/data" `
  -v "${PWD}/.streamlit:/app/.streamlit:ro" `
  rag-docs-bot
```

**Volume mount explanation:**
- `-v "$PWD/data:/app/data"` - Mounts your local data folder (read/write for logs)
- `-v "$PWD/.streamlit:/app/.streamlit:ro"` - Mounts secrets folder (read-only)
- `:ro` flag makes the `.streamlit` folder read-only for security

Browser access
--------------
Open `http://localhost:8501` in your browser (not `http://0.0.0.0:8501`).

Customization
-------------
* **Change documentation source:** Edit `START_URL` and `BASE_DOMAIN` in `crawler.py`
* **Adjust crawl depth:** Modify `MAX_PAGES` in `crawler.py` (default: 200)
* **Change embedding model:** Update `COHERE_EMBED_MODEL` in `.env` before rerunning `build_index.py`
* **Modify retrieval:** Edit `TOP_K` in `app.py` to retrieve more/fewer chunks

Configuration File Summary
--------------------------
| File | Used By | Required For | Format |
|------|---------|--------------|--------|
| `.env` | Everything (local) | Database building + Local Streamlit | `KEY=value` |
| `.streamlit/secrets.toml` | Docker only | Running in Docker | `KEY = "value"` |

**Key Point:** For local development, you ONLY need `.env`. The `.streamlit/secrets.toml` is ONLY required if you want to run with Docker.


Quick Start (TL;DR)
-------------------
**Local development:**
```bash
# Setup
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Create .env with your API keys
cat > .env << EOF
COHERE_API_KEY=your_key
PINECONE_API_KEY=your_key
OPENAI_API_KEY=your_key
PINECONE_INDEX=website-knowledge-index
EOF

# Build database
python crawler.py
python build_index.py

# Run
streamlit run app.py
```

**Docker deployment:**
```bash
# After building database locally, create secrets for Docker
mkdir -p .streamlit
cat > .streamlit/secrets.toml << EOF
COHERE_API_KEY = "your_key"
PINECONE_API_KEY = "your_key"
OPENAI_API_KEY = "your_key"
PINECONE_INDEX = "website-knowledge-index"
EOF

# Run with Docker Compose
docker compose up --build
```
